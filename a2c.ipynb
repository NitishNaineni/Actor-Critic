{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt  \n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from collections import namedtuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class exp_replay():\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, mem_size,  b_size):\n",
    "        self.mem = deque(maxlen=mem_size)\n",
    "        self.b_size = b_size\n",
    "        self.exp_tuple = namedtuple(\"Exp\", field_names=[\"states\",\"actions\", \"reward\", \"next_states\", \"done\"])\n",
    "        \n",
    "        \n",
    "    def append_exp(self, states, actions, rewards, next_states, done):\n",
    "        states = torch.from_numpy(states).type(torch.float)\n",
    "        actions = torch.from_numpy(actions).type(torch.float)\n",
    "        rewards = torch.from_numpy(rewards).type(torch.float)\n",
    "        next_states = torch.from_numpy(next_states).type(torch.float)\n",
    "        done = torch.from_numpy(done).type(torch.float)\n",
    "        exp = self.exp_tuple(states, actions, rewards, next_states, done)\n",
    "        self.mem.append(exp)\n",
    "        \n",
    "    def get_batch(self):\n",
    "        \n",
    "        batch = random.sample(self.mem, self.b_size)\n",
    "        \n",
    "        states_b = torch.stack([i.states for i in batch])\n",
    "        rewards_b = torch.stack([i.rewards for i in batch])\n",
    "        next_states_b = torch.stack([i.next_states for i in batch])\n",
    "        done_b = torch.stack([i.done for i in done])\n",
    "        actions_b = torch.stack([i.actions for i in batch])\n",
    "        return states_b, actions_b, rewards_b, next_states_b, done_b\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_nodes):\n",
    "        super(Policy, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dimput_dim = output_dim\n",
    "        self.hn = hidden_nodes\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_dim, self.hn)\n",
    "        self.fc2 = nn.Linear(self.hn, self.hn)\n",
    "        self.fc3 = nn.Linear(self.hn, self.output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = Categorical(F.softmax(self.fc3(x, dim=1))) # This computes a distribution from which we can sample and take log of for the gradient decent\n",
    "        return x \n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_nodes):\n",
    "        self.input_dim = input_dim\n",
    "        self.hn = hidden_nodes\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_dim, self.hn)\n",
    "        self.fc2 = nn.Linear(self.hn , self.hn)\n",
    "        self.fc3 = nn.Linear(self.hn, 1) # Outputs a Q value of the state\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, epochs, actor, critic):\n",
    "    \n",
    "    actor_optimizer = optim.Adam(actor.parameters())\n",
    "    critic_optimizer = optim.Adam(critic.parameters())\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        obs = env.reset()\n",
    "        \n",
    "        while not done:\n",
    "            obs = torch.from_numpy(obs).type(torch.float)\n",
    "            # Do we need the exploration exploitation?\n",
    "            action_prob  = actor(obs)\n",
    "            action = action_prob.sample()\n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d15a3258ed890393d55d2a553394250e00c408071094df2492a0a7cd6038fb0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
